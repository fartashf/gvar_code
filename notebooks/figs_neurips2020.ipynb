{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Introduce Gluster, formulation, and Gluster objective (details of Gluster algo after experiments)\n",
    "- Random Features experiments. Conclusion: There is a limitted range for both Gluster and SVRG to work.\n",
    "- Which datasets are in that regime?\n",
    "- Positives: extra duplicates added to NQM, MNIST, CIFAR-10, CIFAR-100\n",
    "- Negatives: Total distortions plot as a function of # clusters. CIFAR10-Imagenet-NLP\n",
    "- Details of Gluster algo and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF variance vs overparametrization (mostly ready)\n",
    "* MNIST train/var plots\n",
    "* CIFAR10 var plots and plus noise\n",
    "* NQM, MNIST, CIFAR-10, CIFAR100 +duplicate var plots\n",
    "* CIFAR-10,Imagenet,NLP(bow/bert) Total distortion plots (notice total distortions has Nk multiplier explain and compare to random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Left\n",
    "- CIFAR10 raw data variance plot\n",
    "- Imagenet duplicates var plot (online)\n",
    "- NLP vs C plots\n",
    "- normalized var plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "- Results on Noisy Quadratic Model (linear regression with noise in the labels) are in favor of Gluster.\n",
    "- The issue on CIFAR10 where the Gluster estimator had high variance can be resolved by adding noise to the data. It seems to be a data problem that doesn't appear as strongly on CIFAR-100 and Imagenet. But very specific types of noise can fix that.\n",
    "- Gluster shines with extra duplicates added to NQM, MNIST, CIFAR-10, CIFAR-100. The variance is lower than SGD with double the mini-batch size.\n",
    "- On Imagenet, the gradients do not form clusters, hence no improvement on uniform sampling. For this part, I have a new plot that measures the performance of clustering and how it compares to uniform sampling.\n",
    "- I have the sketch of a proof that for a fixed memory footprint, our rank-1 formulation achieves the lowest clustering objective possible compared to any rank-k formulation.\n",
    "\n",
    "Note that:\n",
    "- With the exception of NQM, I did not use Gluster for training. I measured the variance of Gluster on the trajectory of mini-batch SGD with the same mini-batch size.\n",
    "- Gluster is said to perform well and shine when its variance is lower than SGD with twice the mini-batch size.\n",
    "- In all experiments I used batch Gluster, i.e., I fix the model a few times during the training, I do a number of full assignment/update steps and update the sampling. This version of Gluster is slower but more accurate with fewer hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Wed, Mar 25, 2020 at 12:30 PM David Duvenaud <duvenaud@cs.toronto.edu> wrote:\n",
    "Nice!  A few comments:\n",
    "\n",
    "1) It seems a bit funny to compare to SGD with twice the minibatch size.  Why not just compare with the same minibatch sizes?\n",
    "\n",
    "Good point. That is one of our 3 baselines we compare to:\n",
    "1) SGD with the same mini-batch size: We want to at least beat this. With the exception of CIFAR-10 (without noise) Gluster is always as good or better than this.\n",
    "2) SGD with double the mini-batch size: Gluster has extra memory foot-print for every cluster center equal to the memory taken by the mini-batch. If we are better than 2xB SGD, Gluster is worth the extra memory.\n",
    "3) SVRG: SVRG is only good for training on simple linear regression and MNIST. It also has extra computation cost. But since it is a variance reduction method, it is a natural baseline. On NQM we get very close to SVRG with much less computation cost.\n",
    "\n",
    "2) Maybe another nice demo would be for unbalanced data.  If our method automatically up-weighted the rare classes, that would be another selling point.\n",
    "\n",
    "That's an excellent point.\n",
    "Would it be more interesting if I downsample a few classes from MNIST/CIFAR-10?\n",
    "Or should I create a toy problem similar to NQM but with multiple classes?\n",
    "\n",
    "3) It seems strange that adding noise to the data reduced the variance of our gradient estimator.  Do you have an idea why that could have helped?  I am worried there might be a bug.\n",
    "\n",
    "This is the problem I was stuck at for a year. Let me clarify the problem and my findings.\n",
    "Quick correction: adding noise increases variance as you expected.\n",
    "But I'm not comparing the absolute value of the variance with and without the noise.\n",
    "I comparing Gluster to SGD without noise and then separately Gluster to SGD with noise.\n",
    "\n",
    "Problem:\n",
    "With Gluster, we don't have any guarantee that clustering stays optimal when the model changes during training.\n",
    "In theory, a non-smooth objective function can have very different gradients after a single optimization step.\n",
    "We can constructs examples where Gluster becomes worse than uniform sampling.\n",
    "We have always hoped that this is not true, and the clustering stays valid for a while.\n",
    "Hopefully, longer than the time it takes the control variate of SVRG to become stale.\n",
    "\n",
    "Problem manifestation:\n",
    "On CIFAR-10 however, gradients can change rapidly. See the attached variance plot cifar10_resnet8_smoothing.png (this is with label smoothing, without label smoothing is a bit more spiky).\n",
    "For half the training, the variance of Gluster estimator is lower than SGD but after the learning rate drop, spikes appear.\n",
    "It is very important to observe that the high variance points are spikes. The gradients suddenly become completely different but then they change back again.\n",
    "Geometrically, it can mean the model is oscillating between different basins of attraction. I'm not sure about that.\n",
    "My hypothesis is that, CIFAR-10 is such that the model can overfit to it, but not fully. Maybe because of mislabeled or ambiguous data. But something makes the model change a lot.\n",
    "\n",
    "Solution (or rather making data favourable to Gluster)\n",
    "Again, it is important that the problem is spikes, not consistently high variance as in SVRG. So maybe we can solve it.\n",
    "I thought let's treat the problem as having some sort of non-smoothness and try to smoothen it.\n",
    "So I tried adding different types of noise. In particular, corrupting a portion of labels (changing them to something random) got rid of the spikes (cifar10_resnet8_corrupt.png)\n",
    "\n",
    "With label corruption, I'm changing the task and data but as long as there is not too much corruption, a trained model will generalize to the true data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On when optimal sampling could help: https://arxiv.org/pdf/1412.0156.pdf\n",
    "On when the bias and variance of deep learning models: https://arxiv.org/pdf/2002.11328.pdf\n",
    "\n",
    "These papers have conflicting observations. So I think it might be instructive to run a set of experiments to figure out which of the following regime: overparameterized vs under parameterized vs right at the interpolation threshold, our gluster / optimal sampling data points will work better.\n",
    "\n",
    "I think we should go back to our linear regression and multinomial regression tasks, instead of working with the linear models, we need to run experiments with random kitchen sinks and only learns the top layer weights. The label will also be generated by a teacher random kitchen sink. \n",
    "\n",
    "The point of this exercise is that we can now vary the number of hidden units, d, in a random kitchen sink to simulate the three regimes: overparameterized (d >>n) vs under parameterized (n >>d) vs right at the interpolation threshold (n ~= d). \n",
    "\n",
    "So far we believe that when d >>n, non of the variance reduction methods matter, we verified this hypothesis on linear models but that may not be the same as neural networks, a better study is to look at the random kitchen sink experiments. I suspect that gluster should work well for the under parameterized (n >>d) regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks David for the reminder. I was at his talk and discussed Gluster with him in a meeting.\n",
    "I think their results based on the SGC assumption are closely related. If we our hypothesis holds in RF experiments we can build a theory based on SGC. He was also interested.\n",
    "His slides for that talk:\n",
    "https://www.cs.ubc.ca/~schmidtm/Documents/2020_Vector_SmallResidual.pdf\n",
    "\n",
    "I'm working on the random features/kitchen sink experiments. We should have more definite answers for the underparametrized regime there.\n",
    "\n",
    "By the way, I also tried imbalance data on both CIFAR-10 and MNIST.\n",
    "Again good variance reduction with Gluster on MNIST but fluctuating and high variance on CIFAR-10.\n",
    "This doesn't affect our hypothesis about parametrization.\n",
    "We will be able to do robust optimization in settings like MNIST where Gluster is stable and good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find plots attached for random features models in underparam/overparam regimes. Details are below.\n",
    "Please let me know your thoughts.\n",
    "\n",
    "Conclusions:\n",
    "- It looks like we need more overparametrization for Gluster to work but too much overpamaterization leaves no room for variance reduction.\n",
    "- Gluster seem to be as good as SGD-2B with mild overparametrization (1-4).\n",
    "- Gluster and SVRG are both unstable with large learning rate in underparametrized regime (<=1).\n",
    "- In highly overparametrized regime (10), the gain from SVRG vanishes.\n",
    "- Here are rough overparametrization coefficients I had been working with (not accounting for data augmentation):\n",
    "  + MNIST CNN: 37, MLP: 31  (consistent variance reduction below SGD-2B)\n",
    "  + CIFAR-10 Resnet8: 3, Resnet32: 9 (unstable variance reduction)\n",
    "  + ImageNet ResNet18: 10 (consistently no variance reduction)\n",
    "- If we account for data augmentation, assuming each training data gives us at least 10 new data points, all models I tried on CIFAR-10 and ImageNet are underparametrized.\n",
    "\n",
    "More observations:\n",
    "- The only hyperparameters that affect the variance are learning rate and the ratio of student_hidden/num_train_data. Each dot is an average over all other hyperparameters. Small error bars show that other hyperpameters do not matter.\n",
    "- Contrary to our expectation, the ratio student_hidden/teacher_hidden does not affect the variance of the gradient.\n",
    "- Gluster is always between SGD-B (same mini-batch size) and SGD-2B (double mini-batch size). Almost never worse than SGD-B, never better than SGD-2B.\n",
    "- Gluster has high variance with large learning rate (0.1) in the underparametrized regime and the interpolation point.\n",
    "- The gain of SVRG vanishes in the over-parametrized regime in 2 ways: 1) it provides less variance reduction 2) all methods have relatively low variance.\n",
    "- One bonus plot: the variance of SGD for 3 learning rates together in plot shows that the variance of the gradient is smaller for larger learning rates and the gap grows as overparametrization grows.\n",
    "\n",
    "Experimental setting:\n",
    "- The random features (RF) model is a 2 layer binary classification model where the first layer weights are fixed. Only the second layer weights are trained. The first layer's activation is Relu and the model is trained with cross-entropy loss. Each random feature is sampled from a normal distribution and normalized to L2 norm 1.\n",
    "- Data is generated from a Gaussian and labeled by a teacher RF model.\n",
    "- We train a student RF model on this data.\n",
    "- Important hyperparameters:\n",
    "  + dim: Dimensionality of input\n",
    "  + teacher_hidden\n",
    "  + student_hidden\n",
    "  + num_train_data\n",
    "  + learning rate\n",
    "\n",
    "How to read plots:\n",
    "- There are 3 plots for 3 learning rates (0.1, 0.01, 0.001)\n",
    "- The y-axis (in log-scale) is the mean/max variance over the last 70% of the training (this is to ignore the first epoch where SVRG and Gluster do not have a good variance estimate). Max plot should capture fluctuations of a variance estimator more.\n",
    "- The x-axis is the over-parametrization coefficient (student_hidden/num_train_data). Each point is generated by keeping student_hidden fixed (1000) and varying num_train_data (in the range [0.1, 10]).\n",
    "- We average over different values of the rest of hyperpameters:\n",
    "  + Multiple random seeds (3)\n",
    "  + teacher_hidden (0.1x and 10x student hidden)\n",
    "  + input dim (0.1x and 10x student_hidden)\n",
    "  + momentum=0\n",
    "  + weight decay=1e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
