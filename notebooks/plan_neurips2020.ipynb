{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Introduce Gluster, formulation, and Gluster objective (details of Gluster algo after experiments)\n",
    "- Random Features experiments. Conclusion: There is a limitted range for both Gluster and SVRG to work.\n",
    "- Which datasets are in that regime?\n",
    "- Positives: extra duplicates added to NQM, MNIST, CIFAR-10, CIFAR-100\n",
    "- Negatives: Total distortions plot as a function of # clusters. CIFAR10-Imagenet-NLP\n",
    "- Details of Gluster algo and implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF variance vs overparametrization (mostly ready)\n",
    "* MNIST train/var plots\n",
    "* CIFAR10 var plots and plus noise\n",
    "* NQM, MNIST, CIFAR-10, CIFAR100 +duplicate var plots\n",
    "* CIFAR-10,Imagenet,NLP(bow/bert) Total distortion plots (notice total distortions has Nk multiplier explain and compare to random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* mean to avg\n",
    "* actionable suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## appendix plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF max variance\n",
    "* RF loss/acc plots samples\n",
    "* mnist loss/acc plots\n",
    "* cifar-10 loss/acc plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Left\n",
    "- CIFAR10 raw data variance plot (done, had it)\n",
    "- *Imagenet duplicates var plot (online)\n",
    "- NLP vs C plots (remove)\n",
    "- *NLP var plots\n",
    "- *normalized var plots (in appendix or main?)\n",
    "- svrg in duplicate plots (done)\n",
    "- plotc random *gradients* baseline (can't, random act and out grads only, can't keep the rand for next iter)\n",
    "- any training plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nicolas (June 2)\n",
    "```\n",
    "Here are some comments (in chronological order of the paper, regardless of importance):\n",
    "\n",
    "- When you cite me, please use \"Le Roux\" as my last name (curly brackets in the bib file)\n",
    "- For the link between Fisher and the gradient covariance, there is our recent paper: https://arxiv.org/pdf/1906.07774.pdf . It's also briefly mentioned in James Martens' monograph: https://arxiv.org/abs/1412.1193\n",
    "- \"where the gradient of a point is the point itself\" -> I think I know what you mean but this is poorly phrased.\n",
    "- I don't get the following sentence: \"For any feed-forward network, we can decompose terms in AU updates into a sum of scalars for each layer; henceforth, we drop the layer index for simplicity\". Does that mean the subgradients for each layer can be in different clusters for the same datapoint? You don't enforce consistency of the clustering across layers? If so, it needs to be made explicit.\n",
    "- \"and the RHS is the product of two scalars\": I'm not sure what you mean by that. Also, isn't there a transpose missing?\n",
    "- line 179: \"eigenvectors\" should be \"left and right singular vectors\"\n",
    "- there's only a 4.1, I would remove the subsection title\n",
    "- Eq. 6 implies that you do the clustering within a minibatch, is this correct? I don't think this is ever made explicit.\n",
    "- You don't say whether or not you update the size of each cluster fixed. If you do, then A is a complicated operator.\n",
    "- Relationship between SG-B and SG-2B: these two algorithms take different trajectories, they thus have no reason to have a fixed ratio of variances (see Fig. 2a). At the same parameter \\theta, the normalized variance should also be half for SG-2B. It's unclear to me what you're trying to convey here about the normalized variance.\n",
    "- SGC requires the variance to go to 0, not just to decrease. It doesn't look like it goes to 0 on MNIST.\n",
    "- Your experiments never mention the actual model performance, only the variance of the gradients, which you claim is a surrogate. This is sketchy. People will assume that, since you don't show the results, reducing the variance does not help with convergence speed. Is this true?\n",
    "\n",
    "Hope this helps.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David (June 4)\n",
    "* the link to snr is still valid. 1e-4 and 1e-7 for cifar10 and imagenet doesn't make sense.\n",
    "* but the norm var is going above 1 instead of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David (June 3)\n",
    "* put single trajectory as a paragraph in experiments and the end of intro. it's too much detail for the intro and a single contribution. Explain why this is important in experiments by comparing methods on the same ground.\n",
    "* he likes bold in this paper but not general, in the experiments section it helped a lot.\n",
    "* Some of these contributions are primarily empirical.\n",
    "* This is an opportunity for theorists.\n",
    "* Why should you care.\n",
    "* If you're so smart,explain this.\n",
    "* take care of Nk between A,U. maybe remove alg1. Nk is like mixing probs.\n",
    "* Decouple.\n",
    "* maybe bold 'single traj'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David (May 29)\n",
    "* me: take a look at experiments and tell me what observations you notice\n",
    "* for jimmy: what's the finish if it is a good start?\n",
    "* questions: why are questions important?\n",
    "  - if gradients cluster, we can use the structure and improve optimization\n",
    "  - other stats: there are variety of factors that affect how the convergence changes\n",
    "  - if the answer was this, it could have an impact\n",
    "  - even though this question is important people have not asked it\n",
    "* why learning rate has an effect? 2 prev assumptions: grad goes to zero or constant\n",
    "  - within a basic of attraction, the gradinet should go to zero\n",
    "* I'm using momentum, but it's not used in the estimation, it's not like the variance is over a window, the model is constant\n",
    "* make sure to explain clearly in the related works how this is different from measuring loss in nqm of guodong's. and different noisy directions and different convergence rates.\n",
    "* is there anything that would raise a red flag about the correctness of expeminets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jimmy (May 27)\n",
    "The current paper looks like a good start. For the abstract, usually the abstract is motivated by an open research problem and mentions why the previously it was not solved. Then we can mention our contributions. The last couple of sentences can mention how our contributions address the open research problem.\n",
    "\n",
    "I would avoid words like interesting phenomena, instead, describe what the phenomena are and why they are interesting/unexpected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David D. (May 26)\n",
    "\n",
    "Here's my feedback for the first 1.5 pages, more to come:\n",
    "\n",
    "- Title: I hope we can make a more concrete title\n",
    "- Abstract: \"variance of mini-batch gradients is minimized if the mini-batches\n",
    "are sampled from a weighted clustering in the gradient space.\"  I think this could be made more precise without making it much longer.  Minimized over what set?  Isn't it the mini-batch elements that are sampled?\n",
    "- \"Gradient Clustering (Gluster)\" should it be \"Glustering\"?  TBH, I don't really like the name \"gluster\", I think we should just call it gradient clustering.\n",
    "- \"observe interesting phenomena.\" give examples.\n",
    "- \". We also introduce normalized variance as a promising statistic of the gradient distribution\" what makes a statistic promising, and for what task?\n",
    "\n",
    "- line 9:  IID -> i.i.d. \n",
    "- line 11: \"x comprises inputs and ground-truth labels\" incorrect use of comprise.  Should be \"inputs and labels comprise x\".\n",
    "- line 12: should be empirical risk\n",
    "- Intro para:  I think we can skip explaining gradient descent and start with SGD\n",
    "\n",
    "\n",
    "- \"â€¢ What do gradients of data points look like as points in the gradient space? Do gradients\n",
    "28 have structure? Do they cluster?\" -> Is there structure in the distribution over gradients of standard supervised learning problems?\n",
    "- \"What factors affect the gradient variance? Does it depend on the dataset, learning rate,\n",
    "32 model architecture, mini-batch size, optimization algorithm, or the distance to local optima?\" presumably all of these affect the gradient variance indirectly, although things like the learning rate don't affect it directly.  I think you should ask \"to what extent do these factors affect gradient variance\"?\n",
    "- \"How does the gradient distribution affect optimization? For example, what characteristics of\n",
    "34 the distribution correlate with the convergence speed and the minimum loss reachable on\n",
    "35 training and test set?\"  What do you mean by \"reachable\"?  Do you mean to just say \"reached\"?  I think this is a great question to ask, but reviewers will expect you to also cover related theoretical results.  You need to emphasize that this all empirical.\n",
    "\n",
    "- \"We propose Gradient Clustering (Gluster) as an efficient method for modeling the gradients\n",
    "38 with the objective of minimizing the variance of gradient mean estimate\" need to say something about the computational\n",
    "\n",
    "\n",
    "Fig 1: The caption needs a sentence to say what the takeaway is.  Is it that we can train only using gradient cluster centers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David (May 23)\n",
    "- actionable? mnist don't do anything else, cifar10, double mini-batch is not  gaining. imagenet, why does it start low and goes up? each data contributes  something small as we get closer. why cifar10? large var in large grads.  structure? is there sturcture? gluster one thing,\n",
    "\n",
    "* let's see if there is\n",
    "* questions early in paper, how important reduction in gradient variance in\n",
    "* convergence of dl? that needs train plots. all plots are before that and\n",
    "* none\n",
    "* of them are obvious.\n",
    "\n",
    "* list a bunch of questions in the intro.\n",
    "* significant of data sets combined with grad estims?\n",
    "* people rarely look at the variance.\n",
    "* this paper is just the first step in how variance looks like.\n",
    "* how different is from data set to data set.\n",
    "* does it correlate with anything?\n",
    "\n",
    "* which of the questions are important\n",
    "* which of them are deadends based on plots\n",
    "* they are dataset dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David (May 15)\n",
    "- Audience: hacky people or theory people?\n",
    "- Takeway for each: hacky (you do something, here are limitations), theory ?\n",
    "- Imagenet duplicates, run if you have time\n",
    "- nlp, bert models\n",
    "- this is a paper that needs experiments to be well-explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "- Results on Noisy Quadratic Model (linear regression with noise in the labels) are in favor of Gluster.\n",
    "- The issue on CIFAR10 where the Gluster estimator had high variance can be resolved by adding noise to the data. It seems to be a data problem that doesn't appear as strongly on CIFAR-100 and Imagenet. But very specific types of noise can fix that.\n",
    "- Gluster shines with extra duplicates added to NQM, MNIST, CIFAR-10, CIFAR-100. The variance is lower than SGD with double the mini-batch size.\n",
    "- On Imagenet, the gradients do not form clusters, hence no improvement on uniform sampling. For this part, I have a new plot that measures the performance of clustering and how it compares to uniform sampling.\n",
    "- I have the sketch of a proof that for a fixed memory footprint, our rank-1 formulation achieves the lowest clustering objective possible compared to any rank-k formulation.\n",
    "\n",
    "Note that:\n",
    "- With the exception of NQM, I did not use Gluster for training. I measured the variance of Gluster on the trajectory of mini-batch SGD with the same mini-batch size.\n",
    "- Gluster is said to perform well and shine when its variance is lower than SGD with twice the mini-batch size.\n",
    "- In all experiments I used batch Gluster, i.e., I fix the model a few times during the training, I do a number of full assignment/update steps and update the sampling. This version of Gluster is slower but more accurate with fewer hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Wed, Mar 25, 2020 at 12:30 PM David Duvenaud <duvenaud@cs.toronto.edu> wrote:\n",
    "Nice!  A few comments:\n",
    "\n",
    "1) It seems a bit funny to compare to SGD with twice the minibatch size.  Why not just compare with the same minibatch sizes?\n",
    "\n",
    "Good point. That is one of our 3 baselines we compare to:\n",
    "1) SGD with the same mini-batch size: We want to at least beat this. With the exception of CIFAR-10 (without noise) Gluster is always as good or better than this.\n",
    "2) SGD with double the mini-batch size: Gluster has extra memory foot-print for every cluster center equal to the memory taken by the mini-batch. If we are better than 2xB SGD, Gluster is worth the extra memory.\n",
    "3) SVRG: SVRG is only good for training on simple linear regression and MNIST. It also has extra computation cost. But since it is a variance reduction method, it is a natural baseline. On NQM we get very close to SVRG with much less computation cost.\n",
    "\n",
    "2) Maybe another nice demo would be for unbalanced data.  If our method automatically up-weighted the rare classes, that would be another selling point.\n",
    "\n",
    "That's an excellent point.\n",
    "Would it be more interesting if I downsample a few classes from MNIST/CIFAR-10?\n",
    "Or should I create a toy problem similar to NQM but with multiple classes?\n",
    "\n",
    "3) It seems strange that adding noise to the data reduced the variance of our gradient estimator.  Do you have an idea why that could have helped?  I am worried there might be a bug.\n",
    "\n",
    "This is the problem I was stuck at for a year. Let me clarify the problem and my findings.\n",
    "Quick correction: adding noise increases variance as you expected.\n",
    "But I'm not comparing the absolute value of the variance with and without the noise.\n",
    "I comparing Gluster to SGD without noise and then separately Gluster to SGD with noise.\n",
    "\n",
    "Problem:\n",
    "With Gluster, we don't have any guarantee that clustering stays optimal when the model changes during training.\n",
    "In theory, a non-smooth objective function can have very different gradients after a single optimization step.\n",
    "We can constructs examples where Gluster becomes worse than uniform sampling.\n",
    "We have always hoped that this is not true, and the clustering stays valid for a while.\n",
    "Hopefully, longer than the time it takes the control variate of SVRG to become stale.\n",
    "\n",
    "Problem manifestation:\n",
    "On CIFAR-10 however, gradients can change rapidly. See the attached variance plot cifar10_resnet8_smoothing.png (this is with label smoothing, without label smoothing is a bit more spiky).\n",
    "For half the training, the variance of Gluster estimator is lower than SGD but after the learning rate drop, spikes appear.\n",
    "It is very important to observe that the high variance points are spikes. The gradients suddenly become completely different but then they change back again.\n",
    "Geometrically, it can mean the model is oscillating between different basins of attraction. I'm not sure about that.\n",
    "My hypothesis is that, CIFAR-10 is such that the model can overfit to it, but not fully. Maybe because of mislabeled or ambiguous data. But something makes the model change a lot.\n",
    "\n",
    "Solution (or rather making data favourable to Gluster)\n",
    "Again, it is important that the problem is spikes, not consistently high variance as in SVRG. So maybe we can solve it.\n",
    "I thought let's treat the problem as having some sort of non-smoothness and try to smoothen it.\n",
    "So I tried adding different types of noise. In particular, corrupting a portion of labels (changing them to something random) got rid of the spikes (cifar10_resnet8_corrupt.png)\n",
    "\n",
    "With label corruption, I'm changing the task and data but as long as there is not too much corruption, a trained model will generalize to the true data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On when optimal sampling could help: https://arxiv.org/pdf/1412.0156.pdf\n",
    "On when the bias and variance of deep learning models: https://arxiv.org/pdf/2002.11328.pdf\n",
    "\n",
    "These papers have conflicting observations. So I think it might be instructive to run a set of experiments to figure out which of the following regime: overparameterized vs under parameterized vs right at the interpolation threshold, our gluster / optimal sampling data points will work better.\n",
    "\n",
    "I think we should go back to our linear regression and multinomial regression tasks, instead of working with the linear models, we need to run experiments with random kitchen sinks and only learns the top layer weights. The label will also be generated by a teacher random kitchen sink. \n",
    "\n",
    "The point of this exercise is that we can now vary the number of hidden units, d, in a random kitchen sink to simulate the three regimes: overparameterized (d >>n) vs under parameterized (n >>d) vs right at the interpolation threshold (n ~= d). \n",
    "\n",
    "So far we believe that when d >>n, non of the variance reduction methods matter, we verified this hypothesis on linear models but that may not be the same as neural networks, a better study is to look at the random kitchen sink experiments. I suspect that gluster should work well for the under parameterized (n >>d) regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks David for the reminder. I was at his talk and discussed Gluster with him in a meeting.\n",
    "I think their results based on the SGC assumption are closely related. If we our hypothesis holds in RF experiments we can build a theory based on SGC. He was also interested.\n",
    "His slides for that talk:\n",
    "https://www.cs.ubc.ca/~schmidtm/Documents/2020_Vector_SmallResidual.pdf\n",
    "\n",
    "I'm working on the random features/kitchen sink experiments. We should have more definite answers for the underparametrized regime there.\n",
    "\n",
    "By the way, I also tried imbalance data on both CIFAR-10 and MNIST.\n",
    "Again good variance reduction with Gluster on MNIST but fluctuating and high variance on CIFAR-10.\n",
    "This doesn't affect our hypothesis about parametrization.\n",
    "We will be able to do robust optimization in settings like MNIST where Gluster is stable and good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please find plots attached for random features models in underparam/overparam regimes. Details are below.\n",
    "Please let me know your thoughts.\n",
    "\n",
    "Conclusions:\n",
    "- It looks like we need more overparametrization for Gluster to work but too much overpamaterization leaves no room for variance reduction.\n",
    "- Gluster seem to be as good as SGD-2B with mild overparametrization (1-4).\n",
    "- Gluster and SVRG are both unstable with large learning rate in underparametrized regime (<=1).\n",
    "- In highly overparametrized regime (10), the gain from SVRG vanishes.\n",
    "- Here are rough overparametrization coefficients I had been working with (not accounting for data augmentation):\n",
    "  + MNIST CNN: 37, MLP: 31  (consistent variance reduction below SGD-2B)\n",
    "  + CIFAR-10 Resnet8: 3, Resnet32: 9 (unstable variance reduction)\n",
    "  + ImageNet ResNet18: 10 (consistently no variance reduction)\n",
    "- If we account for data augmentation, assuming each training data gives us at least 10 new data points, all models I tried on CIFAR-10 and ImageNet are underparametrized.\n",
    "\n",
    "More observations:\n",
    "- The only hyperparameters that affect the variance are learning rate and the ratio of student_hidden/num_train_data. Each dot is an average over all other hyperparameters. Small error bars show that other hyperpameters do not matter.\n",
    "- Contrary to our expectation, the ratio student_hidden/teacher_hidden does not affect the variance of the gradient.\n",
    "- Gluster is always between SGD-B (same mini-batch size) and SGD-2B (double mini-batch size). Almost never worse than SGD-B, never better than SGD-2B.\n",
    "- Gluster has high variance with large learning rate (0.1) in the underparametrized regime and the interpolation point.\n",
    "- The gain of SVRG vanishes in the over-parametrized regime in 2 ways: 1) it provides less variance reduction 2) all methods have relatively low variance.\n",
    "- One bonus plot: the variance of SGD for 3 learning rates together in plot shows that the variance of the gradient is smaller for larger learning rates and the gap grows as overparametrization grows.\n",
    "\n",
    "Experimental setting:\n",
    "- The random features (RF) model is a 2 layer binary classification model where the first layer weights are fixed. Only the second layer weights are trained. The first layer's activation is Relu and the model is trained with cross-entropy loss. Each random feature is sampled from a normal distribution and normalized to L2 norm 1.\n",
    "- Data is generated from a Gaussian and labeled by a teacher RF model.\n",
    "- We train a student RF model on this data.\n",
    "- Important hyperparameters:\n",
    "  + dim: Dimensionality of input\n",
    "  + teacher_hidden\n",
    "  + student_hidden\n",
    "  + num_train_data\n",
    "  + learning rate\n",
    "\n",
    "How to read plots:\n",
    "- There are 3 plots for 3 learning rates (0.1, 0.01, 0.001)\n",
    "- The y-axis (in log-scale) is the mean/max variance over the last 70% of the training (this is to ignore the first epoch where SVRG and Gluster do not have a good variance estimate). Max plot should capture fluctuations of a variance estimator more.\n",
    "- The x-axis is the over-parametrization coefficient (student_hidden/num_train_data). Each point is generated by keeping student_hidden fixed (1000) and varying num_train_data (in the range [0.1, 10]).\n",
    "- We average over different values of the rest of hyperpameters:\n",
    "  + Multiple random seeds (3)\n",
    "  + teacher_hidden (0.1x and 10x student hidden)\n",
    "  + input dim (0.1x and 10x student_hidden)\n",
    "  + momentum=0\n",
    "  + weight decay=1e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
